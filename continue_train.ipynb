{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"continue_train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SoKgeJQnH8Nm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650758368155,"user_tz":-480,"elapsed":38177,"user":{"displayName":"Rhodel Quizon","userId":"02225868595946476003"}},"outputId":"d1ba715d-bbb4-4377-b198-4deeecd38097"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","source":["import os\n","os.chdir(\"gdrive/MyDrive/EE298 2022/REQ02\")"],"metadata":{"id":"QeytM_6pIDwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.utils.data\n","import torchvision\n","from PIL import Image\n","from pycocotools.coco import COCO\n","\n","class myOwnDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, annotation, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        self.coco = COCO(annotation)\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","\n","    def __getitem__(self, index):\n","        # Own coco file\n","        coco = self.coco\n","        # Image ID\n","        img_id = self.ids[index]\n","        # List: get annotation id from coco\n","        ann_ids = coco.getAnnIds(imgIds=img_id)\n","        # Dictionary: target coco_annotation file for an image\n","        coco_annotation = coco.loadAnns(ann_ids)\n","        # path for input image\n","        path = coco.loadImgs(img_id)[0]['file_name']\n","        # open the input image\n","        img = Image.open(os.path.join(self.root, path))\n","\n","        # number of objects in the image\n","        num_objs = len(coco_annotation)\n","\n","        # Bounding boxes for objects\n","        # In coco format, bbox = [xmin, ymin, width, height]\n","        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n","        boxes = []\n","        labels=[]\n","        for i in range(num_objs):\n","            xmin = coco_annotation[i]['bbox'][0]\n","            ymin = coco_annotation[i]['bbox'][1]\n","            xmax = xmin + coco_annotation[i]['bbox'][2]\n","            ymax = ymin + coco_annotation[i]['bbox'][3]\n","            label= coco_annotation[i]['category_id']\n","            boxes.append([xmin, ymin, xmax, ymax])\n","            labels.append(label)\n","           \n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels=torch.as_tensor(labels,dtype=torch.int64)\n","        # Labels (In my case, I only one class: target class or background)\n","        #labels=coco.getCatIds(imgIds=img_id)\n","        # Tensorise img_id\n","        img_id = torch.tensor([img_id])\n","        # Size of bbox (Rectangular)\n","        areas = []\n","        for i in range(num_objs):\n","            areas.append(coco_annotation[i]['area'])\n","        areas = torch.as_tensor(areas, dtype=torch.float32)\n","        # Iscrowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        # Annotation is in dictionary format\n","        my_annotation = {}\n","        my_annotation[\"boxes\"] = boxes\n","        my_annotation[\"labels\"] = labels\n","        my_annotation[\"image_id\"] = img_id\n","        my_annotation[\"area\"] = areas\n","        my_annotation[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        return img, my_annotation\n","\n","    def __len__(self):\n","        return len(self.ids)"],"metadata":{"id":"1RtwjFkGIIeZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from engine import train_one_epoch, evaluate\n","import utils\n","\n","\n","def get_transform():\n","    custom_transforms = []\n","    custom_transforms.append(torchvision.transforms.ToTensor())\n","    return torchvision.transforms.Compose(custom_transforms)"],"metadata":{"id":"ESg4xlHSILFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import utils\n","train_data_dir = r\"Images\"\n","test_data_dir=r\"Images\"\n","train_coco = r\"train.json\"\n","test_coco=r\"test.json\"\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","# create own Dataset\n","my_dataset = myOwnDataset(root=train_data_dir,\n","                          annotation=train_coco,\n","                          transforms=get_transform()\n","                          )\n","my_dataset_test = myOwnDataset(root=test_data_dir,\n","                          annotation=test_coco,\n","                          transforms=get_transform()\n","                          )\n","\n","\n","\n","# Batch size\n","train_batch_size = 8\n","\n","# own DataLoader\n","data_loader = torch.utils.data.DataLoader(my_dataset,\n","                                          batch_size=train_batch_size,\n","                                          shuffle=True,\n","                                          num_workers=4,\n","                                          collate_fn=utils.collate_fn)\n","data_loader_test = torch.utils.data.DataLoader(my_dataset_test,\n","                                          batch_size=1,\n","                                          shuffle=False,\n","                                          num_workers=4,\n","                                          collate_fn=utils.collate_fn)"],"metadata":{"id":"638gJHfLIOed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select device (whether GPU or CPU)\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# DataLoader is iterable over Dataset\n","#for imgs, annotations in data_loader:\n","   # imgs = list(img.to(device) for img in imgs)\n","   # annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n","    #print(annotations)"],"metadata":{"id":"IzDP33-IISLO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","def get_model_instance_segmentation(num_classes):\n","    # load an instance segmentation model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    return model\n","\n","def get_model_instance_segmentation_load(num_classes):\n","    # load an instance segmentation model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    return model\n"],"metadata":{"id":"qOMX3qoEIVOZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_checkpoint(model, optimizer, load_path):\n","    checkpoint = torch.load(load_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    \n","    return model, optimizer, epoch\n","  \n","num_classes =4\n","num_epochs = 10\n","model = get_model_instance_segmentation(num_classes)\n","model.to(device)   \n","# parameters\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=0.0005)\n","\n","#len_dataloader = len(data_loader)\n","\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=3,\n","                                               gamma=0.1)\n","\n","\n","model,optimizer,epoch=load_checkpoint(model,optimizer,'trial_withtrain.pth')\n"],"metadata":{"id":"YixLu84HIbP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","    evaluate(model, data_loader_test, device=device)"],"metadata":{"id":"ymBrqUzgIeRj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_checkpoint(model, optimizer, save_path, epoch):\n","    torch.save({\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'epoch': epoch\n","    }, save_path)\n","\n","\n","save_checkpoint(model,optimizer,\"Model.pth\",epoch)"],"metadata":{"id":"KDDbis_56_bd"},"execution_count":null,"outputs":[]}]}